{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "from typing import List, Type, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AbstractModel(ABC):\n",
    "    def __init__(self, words: List[str], docs: List[List[str]]) -> None:\n",
    "        self.words = words\n",
    "        self.docs = docs\n",
    "\n",
    "    @abstractmethod\n",
    "    def build(self) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "\n",
    "class TermCountModel(AbstractModel):\n",
    "    def build(self):\n",
    "        model = np.zeros((len(self.words), len(self.docs)), dtype=int)\n",
    "\n",
    "        for i, word in enumerate(self.words):\n",
    "            for j, doc in enumerate(self.docs):\n",
    "                model[i, j] = doc.count(word)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "class TFIDFModel(TermCountModel):\n",
    "    def build(self):\n",
    "        term_count_model = super().build()\n",
    "        model = np.zeros((len(self.words), len(self.docs)), dtype=float)\n",
    "\n",
    "        for i, word in enumerate(self.words):\n",
    "            for j, doc in enumerate(self.docs):\n",
    "                tf = term_count_model[i, j] / len(doc)\n",
    "                idf = np.log(sum(term_count_model[i] > 0))\n",
    "                model[i, j] = tf * idf\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSI:\n",
    "    \"\"\"Latent Semantic Indexing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, docs: List[str], query: str, model: Type[AbstractModel] = TermCountModel,\n",
    "                 rank_approximation: int = 2, stopwords: List[str] = None,\n",
    "                 ignore_chars=string.punctuation) -> None:\n",
    "        if stopwords is None:\n",
    "            stopwords = []\n",
    "        self.stopwords = stopwords\n",
    "        self.ignore_chars = ignore_chars\n",
    "        self.docs = list(map(self._parse, docs))\n",
    "        self.words = self._get_words()\n",
    "        self.query = self._parse_query(query)\n",
    "        self.model = model\n",
    "        self.rank_approximation = rank_approximation\n",
    "        self.term_doc_matrix = self._build_term_doc_matrix()\n",
    "\n",
    "    def _parse(self, text: str) -> List[str]:\n",
    "        translator = str.maketrans(self.ignore_chars, ' ' * len(self.ignore_chars))\n",
    "        return list(map(str.lower,\n",
    "                        filter(lambda w: w not in self.stopwords,\n",
    "                               text.translate(translator).split())))\n",
    "\n",
    "    def _parse_query(self, query: str) -> np.ndarray:\n",
    "        result = np.zeros(len(self.words))\n",
    "\n",
    "        i = 0\n",
    "        for word in sorted(self._parse(query)):\n",
    "            while word > self.words[i]:\n",
    "                i += 1\n",
    "            if word == self.words[i]:\n",
    "                result[i] += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _get_words(self) -> List[str]:\n",
    "        words = set()\n",
    "\n",
    "        for doc in self.docs:\n",
    "            words = words | set(doc)\n",
    "\n",
    "        return sorted(words)\n",
    "\n",
    "    def _build_term_doc_matrix(self) -> np.ndarray:\n",
    "        model = self.model(self.words, self.docs)\n",
    "        return model.build()\n",
    "\n",
    "    def _svd_with_dimensionality_reduction(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        u, s, v = np.linalg.svd(self.term_doc_matrix)\n",
    "        s = np.diag(s)\n",
    "        k = self.rank_approximation\n",
    "        return u[:, :k], s[:k, :k], v[:, :k]\n",
    "\n",
    "    def process(self) -> np.ndarray:\n",
    "        u_k, s_k, v_k = self._svd_with_dimensionality_reduction()\n",
    "\n",
    "        q = self.query.T @ u_k @ np.linalg.pinv(s_k)\n",
    "        d = self.term_doc_matrix.T @ u_k @ np.linalg.pinv(s_k)\n",
    "\n",
    "        res = np.apply_along_axis(lambda row: self._sim(q, row), axis=1, arr=d)\n",
    "        ranking = np.argsort(-res) + 1\n",
    "        return ranking\n",
    "\n",
    "    @staticmethod\n",
    "    def _sim(x: np.ndarray, y: np.ndarray):\n",
    "        return (x @ y) / (np.linalg.norm(x) * np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_data = json.load(open('articles_porter_mystem.json'))['issue']['articles']\n",
    "documents = []\n",
    "for article in articles_data:\n",
    "    documents.append(article['annotation']['porter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(query):\n",
    "    lsi = LSI(documents, query)\n",
    "    return lsi.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  7  2  1  3  5  4 10  9  8]\n"
     ]
    }
   ],
   "source": [
    "query = 'сследова динамик множеств критическ точек'\n",
    "print(rank(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  5  1  2  7  6  3 10  9  8]\n"
     ]
    }
   ],
   "source": [
    "query = 'реш нестационарн задач для тонк упруг'\n",
    "print(rank(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  5  1  2  7  6  3 10  9  8]\n"
     ]
    }
   ],
   "source": [
    "query = 'реш нестационарн задач для тонк упруг'\n",
    "print(rank(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  1  7  5  6  4  3 10  9  8]\n"
     ]
    }
   ],
   "source": [
    "query = 'стабилизац решен дифференциальн уравнен в гильбертов пространств'\n",
    "print(rank(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
